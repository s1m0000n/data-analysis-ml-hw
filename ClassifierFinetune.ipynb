{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassifierFinetune.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YJRdGwHVgwE"
      },
      "source": [
        "# Анализ эмоциональной окраски предложений из Twitter - метод 2\n",
        "\n",
        "Проект по курсу \"Анализ неструктурированных данных\" кафедры АЯ ВМК МГУ. \n",
        "\n",
        "Задача: сделать классификаторы 2умя методами и оценить их\n",
        "\n",
        "Используемая здесь модель: RuBERT + линейный слой классификатора - метод 2, не рассказанный в этот раз на лекции\n",
        "\n",
        "## Подготовка\n",
        "### Загрузка моделей и PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xT0JwsJVc_o",
        "outputId": "40c72063-e6bd-4825-cf2b-f3af18ad0a79"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-wel4riw3\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-wel4riw3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.0.0rc1 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (2.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (0.0.35)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (0.9.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0rc1) (1.18.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.0.0rc1) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.0.0rc1) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0rc1) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0rc1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0rc1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0rc1) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.0rc1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.0rc1) (0.17.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.0.0rc1-cp36-none-any.whl size=1349450 sha256=639850376243ba69596638ecf62fec1c4c98b387976eea38a76f5c947a21aa19\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6qxc40a3/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n",
            "Collecting git+https://github.com/huggingface/datasets.git\n",
            "  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-a1brp04j\n",
            "  Running command git clone -q https://github.com/huggingface/datasets.git /tmp/pip-req-build-a1brp04j\n",
            "Requirement already satisfied (use --upgrade to upgrade): datasets==1.1.3 from git+https://github.com/huggingface/datasets.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (1.18.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.25.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (2.22.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.70.10)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.3) (2019.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (2020.6.20)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->datasets==1.1.3) (1.15.0)\n",
            "Building wheels for collected packages: datasets\n",
            "  Building wheel for datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasets: filename=datasets-1.1.3-cp36-none-any.whl size=153709 sha256=8b2e6dc3c7ea72b59bb3d07b19400db5ce5ab82172e1b068aa7c3a71bfb3d89f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4m_2xk_j/wheels/3e/af/ff/d1cdb5d0f9cff6eba2042a16b477ada497e23f1a3b6950b928\n",
            "Successfully built datasets\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNBsKjJ3Zo9r",
        "outputId": "8cddf9e4-5a71-44cf-abf4-6862af48f724"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == 'cpu':\n",
        "    print('cpu')\n",
        "else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU6nqtLaWLXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2655e34f-5f6c-4fd1-fca8-41a98d70b808"
      },
      "source": [
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZYaLR8QaYNU"
      },
      "source": [
        "### Загрузка датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guSDnc8taiL8",
        "outputId": "035e1c5a-6a79-44c5-c86e-0bdf7582ee78"
      },
      "source": [
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 13.57 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX0NWv4bZiPu"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSciRsPCaHXp"
      },
      "source": [
        "pos_texts = pd.read_csv('stepik-dl-nlp/datasets/bert_sentiment_analysis/positive.csv', encoding='utf8', sep=';', header=None)\n",
        "neg_texts = pd.read_csv('stepik-dl-nlp/datasets/bert_sentiment_analysis/negative.csv', encoding='utf8', sep=';', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "l6u4123Ja_oe",
        "outputId": "74daac60-cf2a-4852-867c-44379572a1b8"
      },
      "source": [
        "pos_texts.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906692374446080</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>pleease_shut_up</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7569</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906692693221377</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>alinakirpicheva</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11825</td>\n",
              "      <td>59</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>408906695083954177</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>EvgeshaRe</td>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1273</td>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>408906695356973056</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>ikonnikova_21</td>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1549</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>408906761416867842</td>\n",
              "      <td>1386325943</td>\n",
              "      <td>JumpyAlex</td>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>597</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0           1                2   ...  9   10  11\n",
              "0  408906692374446080  1386325927  pleease_shut_up  ...  62  61   0\n",
              "1  408906692693221377  1386325927  alinakirpicheva  ...  59  31   2\n",
              "2  408906695083954177  1386325927        EvgeshaRe  ...  26  27   0\n",
              "3  408906695356973056  1386325927    ikonnikova_21  ...  19  17   0\n",
              "4  408906761416867842  1386325943        JumpyAlex  ...  16  23   1\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEUCXRwpb-zt"
      },
      "source": [
        "### Подготовим твиты и их лейблы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZDl807bLHq"
      },
      "source": [
        "import numpy as np\n",
        "sentences = np.concatenate([pos_texts[3].values, neg_texts[3].values])\n",
        "\n",
        "# Добавляем специальные токены для BERT\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "\n",
        "labels = [[1] for _ in range(pos_texts.shape[0])] + [[0] for _ in range(neg_texts.shape[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql4qR_wxccTC"
      },
      "source": [
        "### Разделим обучающую и тестовую выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ganaeAYbgve"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pey8X72YdAFX"
      },
      "source": [
        "### Токенизируем\n",
        "BERT использует BPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQx_J9ODcgs5",
        "outputId": "04f6a125-31ec-481a-9415-d40d0c689c0e"
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '@', 'im', '_', 'so', '_', 'fa', '##e', 'у', 'нас', 'вообще', 'все', 'слова', 'мил', '##ые', ';', ')', 'вот', 'так', ',', 'сидя', 'в', 'твиттере', ',', 'я', 'под', '##учи', '##ла', 'русски', '##и', ',', 'а', 'ты', '-', 'украин', '##ски', '##и', 'x', '##d', '##d', '##d', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hfS-eTLdTtr"
      },
      "source": [
        "\n",
        "### BERTу нужно предоставить специальный формат входных данных.\n",
        "\n",
        "- input ids: последовательность чисел, отождествляющих каждый токен с его номером в словаре.\n",
        "- labels: вектор из нулей и единиц. В нашем случае нули обозначают негативную эмоциональную окраску, единицы - положительную.\n",
        "- segment mask: (необязательно) последовательность нулей и единиц, которая показывает, состоит ли входной текст из одного или двух предложений. Для случая одного предложения получится вектор из одних нулей. Для двух:\n",
        "- attention mask: (необязательно) последовательность нулей и единиц, где единицы обозначают токены предложения, нули - паддинг."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LPaIQXNcyLj"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(\n",
        "    input_ids,\n",
        "    maxlen=100,\n",
        "    dtype=\"long\",\n",
        "    truncating=\"post\",\n",
        "    padding=\"post\"\n",
        ")\n",
        "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61SqtKFmdrNq"
      },
      "source": [
        "### Создадим валидационную выборку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXs1_P5vdIzC"
      },
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    input_ids, train_gt, \n",
        "    random_state=42,\n",
        "    test_size=0.1\n",
        ")\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(\n",
        "    attention_masks,\n",
        "    input_ids,\n",
        "    random_state=42,\n",
        "    test_size=0.1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M14EgmjXd9Kq"
      },
      "source": [
        "### Переведём всё в тензоры\n",
        "Так же они отправятся на GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQZWx2W5dwk4"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GedkKSeeC3p",
        "outputId": "67c7e927-6ea3-499b-ebdc-e2a77aced5c7"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [0],\n",
              "        [1],\n",
              "        [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB8z-gXAeQ7v"
      },
      "source": [
        "### Создаём загрузчики данных\n",
        "Они нужны, чтобы удобным для PyTorch образом отдавать данные по батчам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PKAS1CQeF04"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    sampler=RandomSampler(train_data),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(\n",
        "    validation_data,\n",
        "    sampler=SequentialSampler(validation_data),\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPjs23KPecmB"
      },
      "source": [
        "## Обучим наш классификатор\n",
        "\n",
        "BERT уже очень много знает + у нас достаточно большой датасет, поэтому обучать будем одну эпоху.\n",
        "\n",
        "\n",
        "\n",
        "### Подготовим метод оптимизации - Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU3M5Ww9eRU2"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atGAtiJ6fua7"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "pjDQkDR9fimu",
        "outputId": "a40a8db3-3bb6-4feb-8de0-bf58cc7369ea"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Будем сохранять loss во время обучения\n",
        "# и рисовать график в режиме реального времени\n",
        "train_loss_set = []\n",
        "train_loss = 0\n",
        "\n",
        "\n",
        "# Обучение\n",
        "# Переводим модель в training mode\n",
        "model.train()\n",
        "\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "    # добавляем батч для вычисления на GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Распаковываем данные из dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # если не сделать .zero_grad(), градиенты будут накапливаться\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    train_loss_set.append(loss[0].item())  \n",
        "    \n",
        "    # Backward pass\n",
        "    loss[0].backward()\n",
        "    \n",
        "    # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
        "    optimizer.step()\n",
        "\n",
        "    # Обновляем loss\n",
        "    train_loss += loss[0].item()\n",
        "    \n",
        "    # Рисуем график\n",
        "    clear_output(True)\n",
        "    plt.plot(train_loss_set)\n",
        "    plt.title(\"Training loss\")\n",
        "    plt.xlabel(\"Batch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    \n",
        "print(\"Loss на обучающей выборке: {0:.5f}\".format(train_loss / len(train_dataloader)))\n",
        "\n",
        "# Валидация\n",
        "model.eval()\n",
        "\n",
        "valid_preds, valid_labels = [], []\n",
        "\n",
        "for batch in validation_dataloader:   \n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    logits = logits[0].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    batch_preds = np.argmax(logits, axis=1)\n",
        "    batch_labels = np.concatenate(label_ids)     \n",
        "    valid_preds.extend(batch_preds)\n",
        "    valid_labels.extend(batch_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss на обучающей выборке: 0.00598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "delIFmOifsWF",
        "outputId": "e9b1d2fd-1cf5-4dec-803c-55570bc8c73b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n",
        "    accuracy_score(valid_labels, valid_preds) * 100\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Процент правильных предсказаний на валидационной выборке: 99.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnBh71x3jwu4"
      },
      "source": [
        "## Протестируем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw-BWZfCfyeY"
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "input_ids = pad_sequences(\n",
        "    input_ids,\n",
        "    maxlen=100,\n",
        "    dtype=\"long\",\n",
        "    truncating=\"post\",\n",
        "    padding=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lP2KD72gDnr"
      },
      "source": [
        "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(test_gt)\n",
        "\n",
        "prediction_data = TensorDataset(\n",
        "    prediction_inputs,\n",
        "    prediction_masks,\n",
        "    prediction_labels\n",
        ")\n",
        "\n",
        "prediction_dataloader = DataLoader(\n",
        "    prediction_data, \n",
        "    sampler=SequentialSampler(prediction_data),\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RH81WV6gGQa"
      },
      "source": [
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "    # добавляем батч для вычисления на GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Распаковываем данные из dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n",
        "    # Это ускорит процесс предсказания меток для тестовых данных.\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n",
        "    logits = logits[0].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Сохраняем предсказанные классы и ground truth\n",
        "    batch_preds = np.argmax(logits, axis=1)\n",
        "    batch_labels = np.concatenate(label_ids)  \n",
        "    test_preds.extend(batch_preds)\n",
        "    test_labels.extend(batch_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC9QjZbcgKNI",
        "outputId": "04350e2d-0b8f-4dde-df36-d5e1a8fb446d"
      },
      "source": [
        "acc_score = accuracy_score(test_labels, test_preds)\n",
        "print('Процент правильных предсказаний на отложенной выборке составил: {0:.2f}%'.format(\n",
        "    acc_score*100\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Процент правильных предсказаний на отложенной выборке составил: 99.98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEQmLjRqp6L-"
      },
      "source": [
        "### Тесты по заданию\n",
        "\n",
        "Нужно оценить на 3ёх метриках\n",
        "\n",
        "Я выбрал\n",
        "\n",
        "- Precision\n",
        "- Recall\n",
        "- Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbKKWFCLqYvc"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "accuracy = acc_score*100\n",
        "precision = precision_score(test_labels, test_preds)\n",
        "recall = recall_score(test_labels, test_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABpONgznq5ZE",
        "outputId": "de0aeb95-28d7-4ff6-de48-652ef78c92fd"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.98089668043086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA_jgSAjrIX6",
        "outputId": "960c7e4d-fb42-4592-e347-25c8d67fe995"
      },
      "source": [
        "precision * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.97385696857026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSV0DQFArKVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "898d05a2-1a9c-47af-ea6f-138de46742fb"
      },
      "source": [
        "recall * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.98837918712414"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_s3RS-jrLtn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}